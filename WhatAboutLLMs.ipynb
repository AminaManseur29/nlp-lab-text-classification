{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb73f41b-f54f-4201-a179-9c738fbcba4e",
   "metadata": {},
   "source": [
    "# What about the LLMs?\n",
    "\n",
    "LLMs are reputed to have revolutionised automatic language processing. Since the introduction of BERT-type models, all language processing applications have been based on LLMs, of varying degrees of sophistication and size. These models are trained on multiple tasks and are therefore capable of performing new tasks without learning, simply from a prompt. This is known as \"zero-shot learning\" because there is no learning phase as such. We are going to test these models on our classification task.\n",
    "\n",
    "Huggingface is a Franco-American company that develops tools for building applications based on Deep Learning. In particular, it hosts the huggingface.co portal, which contains numerous Deep Learning models. These models can be used very easily thanks to the [Transformer] library (https://huggingface.co/docs/transformers/quicktour) developed by HuggingFace.\n",
    "\n",
    "Using a transform model in zero-shot learning with HuggingFace is very simple: [see documentation](https://huggingface.co/tasks/zero-shot-classification)\n",
    "\n",
    "However, you need to choose a suitable model from the list of models compatible with Zero-Shot classification. HuggingFace offers [numerous models](https://huggingface.co/models?pipeline_tag=zero-shot-classification). \n",
    "\n",
    "The classes proposed to the model must also provide sufficient semantic information for the model to understand them.\n",
    "\n",
    "**Question**:\n",
    "\n",
    "* Write a code to classify an example of text from an article in Le Monde using a model transformed using zero-shot learning with the HuggingFace library.\n",
    "* choose a model and explain your choice\n",
    "* choose a formulation for the classes to be predicted\n",
    "* show that the model predicts a class for the text of the article (correct or incorrect, analyse the results)\n",
    "* evaluate the performance of your model on 100 articles (a test set).\n",
    "* note model sizes, processing times and classification results\n",
    "\n",
    "\n",
    "Notes :\n",
    "* make sure that you use the correct Tokenizer when using a model \n",
    "* start testing with a small number of articles and the first 100's of characters for faster experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63530fba-406f-458f-a3ee-17570a41d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir modèle multilingue, essayer avec des modèles de différentes tailles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccc09b-c55a-4bb6-93cf-63278d7f63e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Installation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8b0862-2529-4be5-b923-82be40a07420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.17.0 mpmath-1.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd95fa0-501e-40d4-90bb-5d7f8119abab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.6)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (400 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, mdurl, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.13.0 keras-3.8.0 libclang-18.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.0 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4decc1e9-9d4a-4b40-8e75-05cf4d6eb28d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f804012-f2c2-4814-bc59-e80f941b2fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /opt/conda/lib/python3.12/site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e277ff-ef60-4875-bbb6-13e150ed430a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f9c5e9-9a54-45f7-b05d-e698b7683796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07accaf4-9cca-4b01-ab2b-64b5ce263ea3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd7af38-cc6f-496d-8645-8019934ef5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927746bc-d220-4a26-b42d-4fcf146a076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 08:57:36.208617: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-21 08:57:36.229028: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740128256.251233    1003 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740128256.257967    1003 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-21 08:57:36.281871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4eaa76d-0fe1-4dbc-8ee1-92180c541996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd9b12-4859-4d55-b223-6be580bc32e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importation de la base de données et tirage de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe910b5-949d-455d-b001-e4682f8a693c",
   "metadata": {},
   "outputs": [],
   "source": [
    " articles_df = pd.read_csv('LeMonde2003_9classes.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "417cec6f-1fb1-4295-bcff-33da24549ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the UNE class\n",
    "articles_df = articles_df[articles_df['category'] != 'UNE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "795bbde7-3cd3-4d83-bae5-7be42b8fff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = articles_df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a7f4667-5008-45e4-bb55-9f92bf392dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28179</th>\n",
       "      <td>jean-paul huchon ps a indiqué lundi 22 décembr...</td>\n",
       "      <td>FRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>ce n'était vraiment pas le moment d'être membr...</td>\n",
       "      <td>INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>la compagnie d'assurance suisse a enregistré e...</td>\n",
       "      <td>ENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6773</th>\n",
       "      <td>consacrant l'essentiel de son discours vendred...</td>\n",
       "      <td>INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15693</th>\n",
       "      <td>le nombre de bacheliers pour la session 2003 d...</td>\n",
       "      <td>SOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5638</th>\n",
       "      <td>en choisissant de montrer le film de vincent g...</td>\n",
       "      <td>ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6581</th>\n",
       "      <td>a deux semaines du sommet du g8 d'evian où le ...</td>\n",
       "      <td>INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25112</th>\n",
       "      <td>washington l'organisation américaine de défens...</td>\n",
       "      <td>INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>répétées à rome et créées à la biennale de ven...</td>\n",
       "      <td>ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17947</th>\n",
       "      <td>une série de normes comptables destinées aux e...</td>\n",
       "      <td>ENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text category\n",
       "28179  jean-paul huchon ps a indiqué lundi 22 décembr...      FRA\n",
       "409    ce n'était vraiment pas le moment d'être membr...      INT\n",
       "2692   la compagnie d'assurance suisse a enregistré e...      ENT\n",
       "6773   consacrant l'essentiel de son discours vendred...      INT\n",
       "15693  le nombre de bacheliers pour la session 2003 d...      SOC\n",
       "...                                                  ...      ...\n",
       "5638   en choisissant de montrer le film de vincent g...      ART\n",
       "6581   a deux semaines du sommet du g8 d'evian où le ...      INT\n",
       "25112  washington l'organisation américaine de défens...      INT\n",
       "2572   répétées à rome et créées à la biennale de ven...      ART\n",
       "17947  une série de normes comptables destinées aux e...      ENT\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "758023a4-3b85-4ba7-b541-4e48f99e0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = sample.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479acd39-a5cf-47e8-b3f6-3f933e7a783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SPO', 'ART', 'FRA', 'SOC', 'INT', 'ENT'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35381cc7-d6bc-4c2b-acd4-c663b6750930",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Définition du modèle et des classes à prédire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b5512a6-77d5-4a8f-a98e-5b4016254c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noms des classes à prédire\n",
    "candidate_labels = ['companies', 'international', 'arts', 'society', 'France', 'sports', 'font page articles']\n",
    "\n",
    "# Dictionnaire de correspondance avec les valeurs de la base de données\n",
    "corresp_dict = {\n",
    "    'companies':'ENT',\n",
    "    'international':'INT',\n",
    "    'arts':'ART',\n",
    "    'society':'SOC',\n",
    "    'France':'FRA',\n",
    "    'sports':'SPO',\n",
    "    'font page articles':'UNE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a50313d-606a-4684-b03a-fd1d5c1bf217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5b987-05a6-46c8-8164-fed224808649",
   "metadata": {},
   "source": [
    "## Prédiction des classes sur l'ensemble de test et évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6239b06f-c42c-4b87-8d63-f54da12e32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement: 100%|██████████| 100/100 [03:10<00:00,  1.91s/texte]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "scores = []\n",
    "\n",
    "# Mesure du temps de traitement\n",
    "start_time = time.time()\n",
    "for text in tqdm(texts.to_list(), desc=\"Traitement\", unit=\"texte\"):\n",
    "    result = classifier(text[:100], candidate_labels, multi_label=True)\n",
    "    \n",
    "    # Trouver l'indice du score le plus élevé\n",
    "    max_score_index = result['scores'].index(max(result['scores']))\n",
    "\n",
    "    # Sélectionner le label correspondant et son score\n",
    "    selected_label = result['labels'][max_score_index]\n",
    "    selected_score = result['scores'][max_score_index]\n",
    "\n",
    "    # Ajouter les éléments aux listes\n",
    "    predictions.append(selected_label)\n",
    "    scores.append(selected_score)\n",
    "    \n",
    "# Temps de traitement\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6800314d-34ba-4031-8a10-63df98f1a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time processing: 190.8923 seconds\n"
     ]
    }
   ],
   "source": [
    "# Afficher les résultats\n",
    "print(f\"Time processing: {processing_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d330df74-dd15-4415-99f0-fd3e6f63e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['prediction'] = predictions\n",
    "sample['scores'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4af746a5-e86d-41d5-91e3-91dc7b01af0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>prediction</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28179</th>\n",
       "      <td>jean-paul huchon ps a indiqué lundi 22 décembr...</td>\n",
       "      <td>FRA</td>\n",
       "      <td>companies</td>\n",
       "      <td>0.972328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>ce n'était vraiment pas le moment d'être membr...</td>\n",
       "      <td>INT</td>\n",
       "      <td>international</td>\n",
       "      <td>0.691037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>la compagnie d'assurance suisse a enregistré e...</td>\n",
       "      <td>ENT</td>\n",
       "      <td>companies</td>\n",
       "      <td>0.976389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6773</th>\n",
       "      <td>consacrant l'essentiel de son discours vendred...</td>\n",
       "      <td>INT</td>\n",
       "      <td>arts</td>\n",
       "      <td>0.553660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15693</th>\n",
       "      <td>le nombre de bacheliers pour la session 2003 d...</td>\n",
       "      <td>SOC</td>\n",
       "      <td>France</td>\n",
       "      <td>0.487241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5638</th>\n",
       "      <td>en choisissant de montrer le film de vincent g...</td>\n",
       "      <td>ART</td>\n",
       "      <td>sports</td>\n",
       "      <td>0.997939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6581</th>\n",
       "      <td>a deux semaines du sommet du g8 d'evian où le ...</td>\n",
       "      <td>INT</td>\n",
       "      <td>international</td>\n",
       "      <td>0.992085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25112</th>\n",
       "      <td>washington l'organisation américaine de défens...</td>\n",
       "      <td>INT</td>\n",
       "      <td>international</td>\n",
       "      <td>0.715720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>répétées à rome et créées à la biennale de ven...</td>\n",
       "      <td>ART</td>\n",
       "      <td>international</td>\n",
       "      <td>0.963955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17947</th>\n",
       "      <td>une série de normes comptables destinées aux e...</td>\n",
       "      <td>ENT</td>\n",
       "      <td>companies</td>\n",
       "      <td>0.964002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text category  \\\n",
       "28179  jean-paul huchon ps a indiqué lundi 22 décembr...      FRA   \n",
       "409    ce n'était vraiment pas le moment d'être membr...      INT   \n",
       "2692   la compagnie d'assurance suisse a enregistré e...      ENT   \n",
       "6773   consacrant l'essentiel de son discours vendred...      INT   \n",
       "15693  le nombre de bacheliers pour la session 2003 d...      SOC   \n",
       "...                                                  ...      ...   \n",
       "5638   en choisissant de montrer le film de vincent g...      ART   \n",
       "6581   a deux semaines du sommet du g8 d'evian où le ...      INT   \n",
       "25112  washington l'organisation américaine de défens...      INT   \n",
       "2572   répétées à rome et créées à la biennale de ven...      ART   \n",
       "17947  une série de normes comptables destinées aux e...      ENT   \n",
       "\n",
       "          prediction    scores  \n",
       "28179      companies  0.972328  \n",
       "409    international  0.691037  \n",
       "2692       companies  0.976389  \n",
       "6773            arts  0.553660  \n",
       "15693         France  0.487241  \n",
       "...              ...       ...  \n",
       "5638          sports  0.997939  \n",
       "6581   international  0.992085  \n",
       "25112  international  0.715720  \n",
       "2572   international  0.963955  \n",
       "17947      companies  0.964002  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57690a7c-4735-4381-983e-1fa68f1beb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les valeurs dans la colonne en utilisant le dictionnaire\n",
    "sample['prediction'] = sample['prediction'].replace(corresp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cb16d82-572a-416b-af83-91537a50df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le nombre de classifications correctes\n",
    "count_equal = (sample['category'] == sample['prediction']).sum()\n",
    "total_lines = len(sample)\n",
    "accuracy_percentage = (count_equal / total_lines) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f154a543-e7d5-4dfb-ba28-6a1534a76c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'évaluation des performances du modèle montre que 'category' et 'prediction' sont égales dans 42 cas sur 100 lignes, soit un taux de précision de 42.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"L'évaluation des performances du modèle montre que 'category' et 'prediction' sont égales dans {count_equal} cas sur {total_lines} lignes, soit un taux de précision de {accuracy_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbb1b6-4310-4e6f-ae6e-6a088f1e3b4c",
   "metadata": {},
   "source": [
    "Résultats faibles, essayer de ralonger le nombre de caractères considérés pour la prédiction et essayer d'autres modèles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dec003-cde0-4451-8e75-00f4b7dcc096",
   "metadata": {},
   "source": [
    "## Expériences avec différents modèles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a07a208-5438-4dc6-ac8d-0cdfaac28192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zero_shot_classification(model_name, text_length=100):\n",
    "    \"\"\"\n",
    "    Effectue une classification zero-shot sur un échantillon des données.\n",
    "    \n",
    "    Paramètres:\n",
    "    - df: DataFrame contenant une colonne 'text' et 'category'\n",
    "    - model_name: Nom du modèle à utiliser\n",
    "    - candidate_labels: Liste des labels possibles\n",
    "    - sample_size: Nombre d'échantillons à utiliser (par défaut 100)\n",
    "    - text_length: Nombre de caractères du texte pris en compte (par défaut 100)\n",
    "    \n",
    "    Retourne:\n",
    "    - Un DataFrame avec les prédictions et scores\n",
    "    - Le temps de traitement\n",
    "    - L'accuracy du modèle\n",
    "    \"\"\"\n",
    "    # Tirage de l'échantillon\n",
    "    sample = articles_df.sample(100).copy()\n",
    "    texts = sample['text']\n",
    "\n",
    "    # Définition du modèle\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "    predictions, scores = [], []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for text in tqdm(texts.to_list(), desc=\"Traitement\", unit=\"text\"):\n",
    "        result = classifier(text[:text_length], candidate_labels, multi_label=True)\n",
    "        \n",
    "        max_score_index = result['scores'].index(max(result['scores']))\n",
    "        selected_label = result['labels'][max_score_index]\n",
    "        selected_score = result['scores'][max_score_index]\n",
    "        \n",
    "        predictions.append(selected_label)\n",
    "        scores.append(selected_score)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    sample['prediction'] = predictions\n",
    "    sample['scores'] = scores\n",
    "\n",
    "    # Remplacement des labels selon un dictionnaire de correspondance\n",
    "    sample['prediction'] = sample['prediction'].replace(corresp_dict)\n",
    "    \n",
    "    # Calcul de l'accuracy\n",
    "    count_equal = (sample['category'] == sample['prediction']).sum()\n",
    "    total_lines = len(sample)\n",
    "    accuracy_percentage = (count_equal / total_lines) * 100\n",
    "    \n",
    "    print(f\"Time processing: {processing_time:.4f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy_percentage:.2f}% ({count_equal}/{total_lines})\")\n",
    "    \n",
    "    return sample, processing_time, accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b94fff51-9163-4c2c-98ca-e868e8846755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des modèles à tester\n",
    "AVAILABLE_MODELS = [\n",
    "    \"knowledgator/comprehend_it-base\",\n",
    "    \"facebook/bart-large-mnli\",\n",
    "    \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\", \n",
    "    \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", \n",
    "]\n",
    "\n",
    "# Dictionnaire des tailles des modèles\n",
    "MODEL_SIZES = {\n",
    "    \"knowledgator/comprehend_it-base\": 0.47,\n",
    "    \"facebook/bart-large-mnli\": 1.63,  # En Go\n",
    "    \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\": 0.49,\n",
    "    \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\": 0.49,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23706b14-8b87-44b6-ae11-abb9301ce60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: knowledgator/comprehend_it-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Traitement: 100%|██████████| 100/100 [01:59<00:00,  1.19s/text]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time processing: 119.1703 seconds\n",
      "Accuracy: 42.00% (42/100)\n",
      "Testing model: facebook/bart-large-mnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Traitement: 100%|██████████| 100/100 [03:05<00:00,  1.85s/text]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time processing: 185.1920 seconds\n",
      "Accuracy: 47.00% (47/100)\n",
      "Testing model: MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Traitement: 100%|██████████| 100/100 [01:51<00:00,  1.12s/text]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time processing: 111.9397 seconds\n",
      "Accuracy: 35.00% (35/100)\n",
      "Testing model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Traitement: 100%|██████████| 100/100 [01:55<00:00,  1.15s/text]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time processing: 115.3597 seconds\n",
      "Accuracy: 37.00% (37/100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Liste pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "# Boucle sur les modèles\n",
    "for model in AVAILABLE_MODELS:\n",
    "    print(f\"Testing model: {model}\")\n",
    "    sample_results, time_taken, accuracy = evaluate_zero_shot_classification(model, 100)\n",
    "\n",
    "    # Ajouter les résultats dans la liste\n",
    "    results.append({\n",
    "        \"Modèle\": model,\n",
    "        \"Taille (Go)\": MODEL_SIZES.get(model, \"N/A\"),\n",
    "        \"Temps (s)\": round(time_taken, 2),\n",
    "        \"Accuracy (%)\": round(accuracy, 2)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a23eb4c0-ba88-4e49-8ead-7dc252693ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Modèle  Taille (Go)  Temps (s)  \\\n",
      "1                      facebook/bart-large-mnli         1.63     185.19   \n",
      "0               knowledgator/comprehend_it-base         0.47     119.17   \n",
      "3       MoritzLaurer/mDeBERTa-v3-base-mnli-xnli         0.49     115.36   \n",
      "2  MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli         0.49     111.94   \n",
      "\n",
      "   Accuracy (%)  \n",
      "1          47.0  \n",
      "0          42.0  \n",
      "3          37.0  \n",
      "2          35.0  \n"
     ]
    }
   ],
   "source": [
    "# Convertir en DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values(by=\"Accuracy (%)\", ascending=False)\n",
    "\n",
    "# Afficher le tableau\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6d790-9044-4370-8fdb-12c1adc43b30",
   "metadata": {},
   "source": [
    "Le modèle facebook/bart-large-mnli a un taux de précision plus élevé que les autres modèles, sa taille et son temps d'exécution sont également les plus élevés. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d8794-a7e1-4301-9eaa-7733a47c7a77",
   "metadata": {},
   "source": [
    "## Evaluation avec plus de mots dans le contexte pour la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d27e4775-6a03-4342-9561-f273294b1fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Traitement: 100%|██████████| 100/100 [04:03<00:00,  2.43s/text]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time processing: 243.1157 seconds\n",
      "Accuracy: 58.00% (58/100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_results, time_taken, accuracy = evaluate_zero_shot_classification(\"facebook/bart-large-mnli\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb120d-bda2-4c34-9cda-da952fad3a22",
   "metadata": {},
   "source": [
    "Meilleurs résultats avec 300 caractères au lieu de 100 pour prédire la classe. 58% de bonnes prédictions au lieu de 47% précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59f0e7f0-3278-496d-abf4-1057e8f5f4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Traitement: 100%|██████████| 100/100 [09:56<00:00,  5.96s/text]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time processing: 596.2937 seconds\n",
      "Accuracy: 35.00% (35/100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_results, time_taken, accuracy = evaluate_zero_shot_classification(\"facebook/bart-large-mnli\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da40dd09-8e66-4128-821d-071dbf04883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Traitement: 100%|██████████| 100/100 [03:47<00:00,  2.28s/text]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time processing: 227.8575 seconds\n",
      "Accuracy: 47.00% (47/100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (3393591218.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[56], line 37\u001b[0;36m\u001b[0m\n\u001b[0;31m    return sample, processing_time, accuracy_percentage\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Tirage de l'échantillon\n",
    "sample = articles_df.sample(100).copy()\n",
    "texts = sample['text']\n",
    "\n",
    "# Définition du modèle\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "predictions, scores = [], []\n",
    "\n",
    "start_time = time.time()\n",
    "for text in tqdm(texts.to_list(), desc=\"Traitement\", unit=\"text\"):\n",
    "    result = classifier(text[:500], candidate_labels, multi_label=True)\n",
    "    \n",
    "    max_score_index = result['scores'].index(max(result['scores']))\n",
    "    selected_label = result['labels'][max_score_index]\n",
    "    selected_score = result['scores'][max_score_index]\n",
    "    \n",
    "    predictions.append(selected_label)\n",
    "    scores.append(selected_score)\n",
    "\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "sample['prediction'] = predictions\n",
    "sample['scores'] = scores\n",
    "\n",
    "# Remplacement des labels selon un dictionnaire de correspondance\n",
    "sample['prediction'] = sample['prediction'].replace(corresp_dict)\n",
    "\n",
    "# Calcul de l'accuracy\n",
    "count_equal = (sample['category'] == sample['prediction']).sum()\n",
    "total_lines = len(sample)\n",
    "accuracy_percentage = (count_equal / total_lines) * 100\n",
    "\n",
    "print(f\"Time processing: {processing_time:.4f} seconds\")\n",
    "print(f\"Accuracy: {accuracy_percentage:.2f}% ({count_equal}/{total_lines})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccf047-f340-420d-a536-57853fe92ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
